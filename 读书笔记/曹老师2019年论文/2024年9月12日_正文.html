<!DOCTYPE html> <html><head>
		<title>2024年9月12日_正文</title>
		<base href="..\../">
		<meta id="root-path" root-path="..\../">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=1.0, maximum-scale=5.0">
		<meta charset="UTF-8">
		<meta name="description" content="主仓库 - 2024年9月12日_正文">
		<meta property="og:title" content="2024年9月12日_正文">
		<meta property="og:description" content="主仓库 - 2024年9月12日_正文">
		<meta property="og:type" content="website">
		<meta property="og:url" content="读书笔记/曹老师2019年论文/2024年9月12日_正文.html">
		<meta property="og:image" content="读书笔记\曹老师2019年论文\assets\c794693ecb2673dc13277a64a5570d9d_md5.png">
		<meta property="og:site_name" content="主仓库">
		<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="lib/rss.xml"><script async="" id="webpage-script" src="lib/scripts/webpage.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script type="module" async="" id="graph-view-script" src="lib/scripts/graph-view.js"></script><script async="" id="graph-wasm-script" src="lib/scripts/graph-wasm.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="graph-render-worker-script" src="lib/scripts/graph-render-worker.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="tinycolor-script" src="lib/scripts/tinycolor.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="pixi-script" src="lib/scripts/pixi.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="minisearch-script" src="lib/scripts/minisearch.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><link rel="icon" href="lib/media/favicon.png"><script async="" id="graph-data-script" src="lib/scripts/graph-data.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><style>body{--line-width:40em;--line-width-adaptive:40em;--file-line-width:40em;--sidebar-width:min(20em, 80vw);--collapse-arrow-size:11px;--tree-horizontal-spacing:0.6em;--tree-vertical-spacing:0.6em;--sidebar-margin:12px}.sidebar{height:100%;min-width:calc(var(--sidebar-width) + var(--divider-width-hover));max-width:calc(var(--sidebar-width) + var(--divider-width-hover));font-size:14px;z-index:10;position:relative;overflow:hidden;transition:min-width ease-in-out,max-width ease-in-out;transition-duration:.2s;contain:size}.sidebar-left{left:0}.sidebar-right{right:0}.sidebar.is-collapsed{min-width:0;max-width:0}body.floating-sidebars .sidebar{position:absolute}.sidebar-content{height:100%;min-width:calc(var(--sidebar-width) - var(--divider-width-hover));top:0;padding:var(--sidebar-margin);padding-top:4em;line-height:var(--line-height-tight);background-color:var(--background-secondary);transition:background-color,border-right,border-left,box-shadow;transition-duration:var(--color-fade-speed);transition-timing-function:ease-in-out;position:absolute;display:flex;flex-direction:column}.sidebar:not(.is-collapsed) .sidebar-content{min-width:calc(max(100%,var(--sidebar-width)) - 3px);max-width:calc(max(100%,var(--sidebar-width)) - 3px)}.sidebar-left .sidebar-content{left:0;border-top-right-radius:var(--radius-l);border-bottom-right-radius:var(--radius-l)}.sidebar-right .sidebar-content{right:0;border-top-left-radius:var(--radius-l);border-bottom-left-radius:var(--radius-l)}.sidebar:has(.sidebar-content:empty):has(.topbar-content:empty){display:none}.sidebar-topbar{height:2em;width:var(--sidebar-width);top:var(--sidebar-margin);padding-inline:var(--sidebar-margin);z-index:1;position:fixed;display:flex;align-items:center;transition:width ease-in-out;transition-duration:inherit}.sidebar.is-collapsed .sidebar-topbar{width:calc(2.3em + var(--sidebar-margin) * 2)}.sidebar .sidebar-topbar.is-collapsed{width:0}.sidebar-left .sidebar-topbar{left:0}.sidebar-right .sidebar-topbar{right:0}.topbar-content{overflow:hidden;overflow:clip;width:100%;height:100%;display:flex;align-items:center;transition:inherit}.sidebar.is-collapsed .topbar-content{width:0;transition:inherit}.clickable-icon.sidebar-collapse-icon{background-color:transparent;color:var(--icon-color-focused);padding:0!important;margin:0!important;height:100%!important;width:2.3em!important;margin-inline:0.14em!important;position:absolute}.sidebar-left .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);right:var(--sidebar-margin)}.sidebar-right .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);left:var(--sidebar-margin)}.clickable-icon.sidebar-collapse-icon svg.svg-icon{width:100%;height:100%}.sidebar-section-header{margin:0 0 1em 0;text-transform:uppercase;letter-spacing:.06em;font-weight:600}body{transition:background-color var(--color-fade-speed) ease-in-out}.webpage-container{display:flex;flex-direction:row;height:100%;width:100%;align-items:stretch;justify-content:center}.document-container{opacity:1;flex-basis:100%;max-width:100%;width:100%;height:100%;display:flex;flex-direction:column;align-items:center;transition:opacity .2s ease-in-out;contain:inline-size}.hide{opacity:0;transition:opacity .2s ease-in-out}.document-container>.markdown-preview-view{margin:var(--sidebar-margin);margin-bottom:0;width:100%;width:-webkit-fill-available;width:-moz-available;width:fill-available;background-color:var(--background-primary);transition:background-color var(--color-fade-speed) ease-in-out;border-top-right-radius:var(--window-radius,var(--radius-m));border-top-left-radius:var(--window-radius,var(--radius-m));overflow-x:hidden!important;overflow-y:auto!important;display:flex!important;flex-direction:column!important;align-items:center!important;contain:inline-size}.document-container>.markdown-preview-view>.markdown-preview-sizer{padding-bottom:80vh!important;width:100%!important;max-width:var(--line-width)!important;flex-basis:var(--line-width)!important;transition:background-color var(--color-fade-speed) ease-in-out;contain:inline-size}.markdown-rendered img:not([width]),.view-content img:not([width]){max-width:100%;outline:0}.document-container>.view-content.embed{display:flex;padding:1em;height:100%;width:100%;align-items:center;justify-content:center}.document-container>.view-content.embed>*{max-width:100%;max-height:100%;object-fit:contain}:has(> :is(.math,table)){overflow-x:auto!important}.document-container>.view-content{overflow-x:auto;contain:content;padding:0;margin:0;height:100%}.scroll-highlight{position:absolute;width:100%;height:100%;pointer-events:none;z-index:1000;background-color:hsla(var(--color-accent-hsl),.25);opacity:0;padding:1em;inset:50%;translate:-50% -50%;border-radius:var(--radius-s)}</style><script defer="">async function loadIncludes(){if("file:"!=location.protocol){let e=document.querySelectorAll("include");for(let t=0;t<e.length;t++){let o=e[t],l=o.getAttribute("src");try{const e=await fetch(l);if(!e.ok){console.log("Could not include file: "+l),o?.remove();continue}let t=await e.text(),n=document.createRange().createContextualFragment(t),i=Array.from(n.children);for(let e of i)e.classList.add("hide"),e.style.transition="opacity 0.5s ease-in-out",setTimeout((()=>{e.classList.remove("hide")}),10);o.before(n),o.remove(),console.log("Included file: "+l)}catch(e){o?.remove(),console.log("Could not include file: "+l,e);continue}}}else{if(document.querySelectorAll("include").length>0){var e=document.createElement("div");e.id="error",e.textContent="Web server exports must be hosted on an http / web server to be viewed correctly.",e.style.position="fixed",e.style.top="50%",e.style.left="50%",e.style.transform="translate(-50%, -50%)",e.style.fontSize="1.5em",e.style.fontWeight="bold",e.style.textAlign="center",document.body.appendChild(e),document.querySelector(".document-container")?.classList.remove("hide")}}}document.addEventListener("DOMContentLoaded",(()=>{loadIncludes()}));let isFileProtocol="file:"==location.protocol;function waitLoadScripts(e,t){let o=e.map((e=>document.getElementById(e+"-script"))),l=0;!function e(){let n=o[l];l++,n&&"true"!=n.getAttribute("loaded")||l<o.length&&e(),l<o.length?n.addEventListener("load",e):t()}()}</script><link rel="stylesheet" href="lib/styles/obsidian.css"><link rel="preload" href="lib/styles/other-plugins.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/other-plugins.css"></noscript><link rel="stylesheet" href="lib/styles/theme.css"><link rel="preload" href="lib/styles/global-variable-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/global-variable-styles.css"></noscript><link rel="preload" href="lib/styles/supported-plugins.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/supported-plugins.css"></noscript><link rel="preload" href="lib/styles/main-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/main-styles.css"></noscript><link rel="preload" href="lib/styles/snippets.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/snippets.css"></noscript></head><body class="publish css-settings-manager theme-dark show-inline-title show-ribbon colorful-folder card-layout-open-light theme-light-background-adapt accent-color-override-light mod-left-split-background-primary-light mod-right-split-background-primary-light mod-root-split-background-primary-light background-underlying-default-light activated-file-default-light card-highlight-light card-layout-open-dark theme-dark-background-darker mod-left-split-background-primary-dark mod-right-split-background-primary-dark mod-root-split-background-primary-dark background-underlying-default-dark activated-file-default-dark codeblock-style-customize callout-style-customize seamless-embeds DB-table-bg-color-adapt Projects-bg-color-adapt"><script defer="">let theme=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");"dark"==theme?(document.body.classList.add("theme-dark"),document.body.classList.remove("theme-light")):(document.body.classList.add("theme-light"),document.body.classList.remove("theme-dark")),window.innerWidth<480?document.body.classList.add("is-phone"):window.innerWidth<768?document.body.classList.add("is-tablet"):window.innerWidth<1024?document.body.classList.add("is-small-screen"):document.body.classList.add("is-large-screen")</script><div class="webpage-container workspace"><div class="sidebar-left sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"><label class="theme-toggle-container" for="theme_toggle"><input class="theme-toggle-input" type="checkbox" id="theme_toggle"><div class="toggle-background"></div></label></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="search-input-container"><input enterkeyhint="search" type="search" spellcheck="false" placeholder="Search..."><div class="search-input-clear-button" aria-label="Clear search"></div></div><include src="lib/html/file-tree.html"></include></div><script defer="">let ls = document.querySelector(".sidebar-left"); ls.classList.add("is-collapsed"); if (window.innerWidth > 768) ls.classList.remove("is-collapsed"); ls.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-left-width"));</script></div><div class="document-container markdown-reading-view hide"><div class="markdown-preview-view markdown-rendered allow-fold-headings allow-fold-lists is-readable-line-width"><style id="MJX-CHTML-styles"></style><pre class="frontmatter language-yaml" tabindex="0" style="display: none;"><code class="language-yaml is-loaded"><span class="token key atrule">tags</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> 论文
  <span class="token punctuation">-</span> 直接存储
  <span class="token punctuation">-</span> 存储
<span class="token key atrule">收藏</span><span class="token punctuation">:</span> <span class="token boolean important">true</span></code><button class="copy-code-button">复制</button></pre><div class="markdown-preview-sizer markdown-preview-section"><h1 class="page-title heading inline-title" id="用于大规模键值存储的 LSM 树托管存储"><p dir="auto">用于大规模键值存储的 LSM 树托管存储</p></h1><div class="heading-wrapper"><div class="heading-children"><div><p dir="auto">Hong Jiang University of Texas at Arlington Arlington, USA <a data-tooltip-position="top" aria-label="mailto:hong.jiang@uta.edu" rel="noopener" class="external-link" href="mailto:hong.jiang@uta.edu" target="_blank">hong.jiang@uta.edu</a></p></div><div><p dir="auto">Lei Tian Tintri Mountain View, USA <a data-tooltip-position="top" aria-label="mailto:leitian.hust@gmail.com" rel="noopener" class="external-link" href="mailto:leitian.hust@gmail.com" target="_blank">leitian.hust@gmail.com</a></p></div><div><p dir="auto">Fei Mei Qiang Cao* KLISS, WNLO, Huazhong University of Sci. and Tech. Wuhan, China {meifei, caoqiang}@hust.edu.cn</p></div><div class="heading-wrapper"><h3 data-heading="KEYWORDS" dir="auto" class="heading" id="KEYWORDS"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>KEYWORDS</h3><div class="heading-children"></div></div><div class="heading-wrapper"><h3 data-heading="抽象" dir="auto" class="heading" id="抽象"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>&nbsp;抽象</h3><div class="heading-children"><div><p dir="auto">LSM 树、键值存储、文件系统性能、应用程序托管存储</p></div></div></div><div class="heading-wrapper"><h3 data-heading="ACM Reference Format: Fei Mei, Qiang Cao, Hong Jiang, and Lei Tian. 2017. LSM-tree" dir="auto" class="heading" id="ACM_Reference_Format:_Fei_Mei,_Qiang_Cao,_Hong_Jiang,_and_Lei_Tian._2017._LSM-tree"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>ACM Reference Format: Fei Mei, Qiang Cao, Hong Jiang, and Lei Tian. 2017. LSM-tree</h3><div class="heading-children"><div><p dir="auto">用于大规模键值存储的托管存储。在 SoCC 17 会议记录中，美国加利福尼亚州圣克拉拉，9 月 24 日至 27 日，201715页 <a rel="noopener" class="external-link" href="https://doi.org/10.1145/3127479.3127486" target="_blank">https://doi.org/10.1145/3127479.3127486</a></p></div><div><p dir="auto">键值存储越来越多地采用 LSM 树作为后端存储中的支持数据结构，并通过文件系统持久化其集群数据。文件系统不仅需要提供文件/目录抽象来组织数据，而且还要保留 LSM 树的关键优势，即物理设备上的顺序和聚合 I/O 模式。不幸的是，我们深入的实验分析表明，从数据布局和 I/O 处理的角度来看，<mark>LSM 树的一些好处可以被底层文件级索引完全抵消。</mark>因此，LSM 树的写入性能保持在远低于存储设备提供的顺序带宽所承诺的水平。在本文中，我们解决了这个问题并提出了 LDS，这是一种基于 LSM 树的直接存储系统，它通过利用 LSM 树结构的写时复制特性来管理存储空间并提供简化的一致性控制，从而充分利用 LSM 树的好处。在 LDS 上运行 LSM 树作为比较的基线，<br>
</p></div></div></div><div class="heading-wrapper"><h3 data-heading="1引言" dir="auto" class="heading" id="1引言"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>&nbsp;1引言</h3><div class="heading-children"><div><p dir="auto">日志结构合并树（LSM 树）已应用于大规模键值存储的本地和分布式环境，例如 LevelDB [19]、RocksDB [16]、HBase [66]、BigTable [7]、Cassandra [32]、PNUTS [11]、InfluxDB [23] 等，<mark>因为 LSM 树能够缓冲内存中的随机写入，然后对持久存储执行顺序写入</mark>，这是硬盘驱动器和固态设备的最佳预期访问模式 [22， 28, 43, 60].为了从 LSM 树的这些潜在优势中受益，其他传统上在 B 树中组织数据的流行数据库存储后端系统也开始在其新版本中使用 LSM 树，例如 MongoDB [13] 和 SQLite4 [61]。理想情况下，LSM 树希望将其数据存储到 storage</p></div><div><p dir="auto">我们分别在三个具有 HDD 和 SSD 的代表性文件系统（EXT4、F2FS、BTRFS）上评估 LSM 树，以研究 LSM 树的性能潜力。评估结果表明，通过采用<mark>LDS 的 LSM 树友好数据布局</mark>，可以将 LSM 树的写入吞吐量从 from&nbsp;1.8X1.8X&nbsp;提高到&nbsp;3X3X&nbsp;HDD 上，从 1.3 倍提高到&nbsp;2.0×2.0×&nbsp;SSD 上的 SSD 上的</p></div></div></div><div class="heading-wrapper"><h3 data-heading="CCS CONCEPTS" dir="auto" class="heading" id="CCS_CONCEPTS"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>CCS CONCEPTS</h3><div class="heading-children"><div><p dir="auto">信息系统 开槽页;分层存储管理space 的 SPACE，但管理存储的文件系统可以通过文件系统索引来防止这种情况发生，如图 1 中所示的示例所示。通常文件系统将文件索引以文件元数据（即 inode）和资源分配映射（即块位图）的形式存储，以分别定位文件/目录数据块和寻找空闲块来存储要写入的数据。一方面，维护这些索引块会产生更多的非顺序 I/O，这会损害 HDD 和 SSD 的性能 [22， 28， 35， 43， 46， 60]。此外，所有这些索引块必须与数据块同步更新，以实现严格的数据一致性，这需要大量的额外工作来执行 [8， 9， 18， 21， 31， 51]。为了解决这些问题并充分利用</p></div><div><p dir="auto">LSM 树，我们介绍了 LDS，这是一种 LSM 树意识的键值存储系统，它将 LSM 树数据直接映射到块存储空间上，无需额外索构的写时复制特性来管理数据一致性，以避免与引即可保留预期的顺序写入访问模式，并通过利用固有的索引机制和 LSM 树结一致性执行相关的开销因此， LDS 完全消除了复杂且</p></div><div><hr></div><div><p dir="auto"></p><div src="读书笔记/曹老师2019年论文/assets/c794693ecb2673dc13277a64a5570d9d_md5.png" class="internal-embed media-embed image-embed is-loaded"><img src="读书笔记/曹老师2019年论文/assets/c794693ecb2673dc13277a64a5570d9d_md5.png"></div><p dir="auto"></p></div><div><p dir="auto">图 1：LSM 树中预期的顺序模式如何被块存储上的文件系统分解的示例。请注意，inode 和分配映射是由不同的文件系统放置和更新的</p></div><div><p dir="auto">昂贵的文件级索引操作，并大大减少了 I/O 的数量，并强烈保留了磁盘写入顺序性。LDS 的这些优势适用于 HDD 和 SSD。我们基于 LevelDB 实现一个 LDS 原型，以</p></div><div><p dir="auto">与通过三种具有不同数据布局和 I/O 处理特性的代表性文件系统存储 LSM 树相比，评估直接存储 LSM 树的好处，即 ext4（就地更新）[40]、f2fs（基于日志，无需漫游树更新）[33] 和 btrfs（Btree 结构化和写入时复制）[55]。实验结果表明， LDS 在检查的所有工作负载下始终如一地显著提高写入性能。在 HDD 上。写入吞吐量至少提高了&nbsp;1.8×1.8×&nbsp;，最高可达&nbsp;3X3X&nbsp;。在 SSD 上，写入吞吐量至少提高了&nbsp;1.3×1.3×&nbsp;X X ，并且由于&nbsp;2.5×.2.5×.&nbsp;索引 LSM 树数据的快捷方式，读取性能也受益于 LDS 设计。本文的其余部分组织如下。在第 2 节中</p></div><div><p dir="auto">我们提供了必要的背景和对文件系统上运行的 LSM 树的深入分析，以激励我们的 LDS 研究。LDS 的设计和实施在第 3 节中有详细说明。相关工作在第 5 节中介绍。最后，我们在第 6 节中结束我们的工作。</p></div></div></div><div class="heading-wrapper"><h2 data-heading="2背景和动机" dir="auto" class="heading" id="2背景和动机"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>2背景和动机</h2><div class="heading-children"></div></div><div class="heading-wrapper"><h2 data-heading="2.1 LSM-Tree and LevelDB" dir="auto" class="heading" id="2.1_LSM-Tree_and_LevelDB"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>2.1 LSM-Tree and LevelDB</h2><div class="heading-children"><div><p dir="auto">一个标准的 LSM 树由一系列容量呈指数级增长的组件&nbsp;C0C0​&nbsp;C1,…,CKC1​,…,CK​&nbsp;组成 [46]，其中&nbsp;C0C0​&nbsp;驻留在内存中，而所有其他组件都驻留在磁盘上，并且组件中的所有键都保持排序以便快速检索。每次数据&nbsp;CxCx&nbsp;量达到其容量限制时，组件的数据都会滚动合并到&nbsp;Cx+1Cx+1​&nbsp;为了分摊合并成本，最先进的 LSM 树</p></div><div><p dir="auto">基于键值存储将每个磁盘上的组件拆分为 chunk [16， 19]，并且仅在组件大小达到限制时部分合并一个组件（一个或多个 chunk），该函数称为 compaction1。具体来说，对&nbsp;CxCx​</p></div><div><p dir="auto">将以循环方式选择目标块 [19]，并将其合并到&nbsp;Cx+1Cx+1​&nbsp;中。与目标 chunk&nbsp;重叠的所有 chunk&nbsp;Cx+1Cx+1​&nbsp;都将被读出以参与合并排序过程。压缩后，将从合并生成的新数据块写入&nbsp;Cx+1Cx+1​&nbsp;，并删除参与合并的过时数据块。从 LSM 树的角度来看，chunk 写入操作可以看作是写时复制过程：新 chunk 中每个键值对的副本都存在于过时的 chunk 中。换句话说，可以恢复和重做块的中断写入操作。因为在所有新 chunk 都被安全持久化之前，不能删除原始 chunk。除了就地写入块之外，LSM 树还会生成预写日志到磁盘备份，以使组件&nbsp;C0C0​&nbsp;在崩溃时可恢复让我们以 LevelDB，一个广泛使用的 LSM 树键值</p></div><div><p dir="auto">store 基于上面介绍的部分合并，作为一个具体示例。图 2 演示了 LevelDB 的结构，其中&nbsp;C0C0​&nbsp;由两个排序的跳过列表（MemTable 和 ImmTable）组成，每个磁盘组件称为一个级别（&nbsp;L0∼L3L0​∼L3​&nbsp;图中为），其中包含多个块（排序字符串表，或 SST）。SST 包括一个排序的键值对正文和一个尾部，该尾部将读取请求索引到正文。解码尾部总是从其最后一个字节（LevelDB 术语中的页脚）开始写入请求首先附加到备份日志，然后插入到 MemTable，如果其大小达到其容量限制，该 MemTable 将被标记为不可变 （ImmTable）。压缩&nbsp;C0C0​&nbsp;将 ImmTable 作为&nbsp;L0L0​&nbsp;块转储到磁盘上，<mark>而磁盘上组件&nbsp;Lrl​&nbsp;上的压缩会将其 SST 之一合并到&nbsp;.</mark>&nbsp;Ln+1​&nbsp;维护一个单独的结构来跟踪所有 SST 的元数据，称为 LSM 索引（即版本），由 MANIFEST 支持。<mark>每个 SST 都有一个唯一的 ID，该 ID 与其元数据一起记录在 MANIFEST 中。对关卡结构进行更改的压缩必须更新 MANIFEST，这也是以日志记录方式实现的，称为 version edit 或 △version。</mark></p></div></div></div><div class="heading-wrapper"><h2 data-heading="2.2文件系统上的 LSM-Tree" dir="auto" class="heading" id="2.2文件系统上的_LSM-Tree"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>2.2文件系统上的 LSM-Tree</h2><div class="heading-children"><div><p dir="auto">通常，基于 LSM 树的本地键值存储（如 LevelDB）通过文件系统将数据<mark>持久化到存储中</mark>，在本文中称为 LSM-on-FS。在 LSM-on-FS 实现中，所有数据（例如，块、日志等）都以文件的形式存储。直观地说，文件系统应该使 LSM 树数据<mark>存储在大型顺序 I/O 中</mark>，这对于 HDD 和 SDD 等低级存储设备来说是一个理想的属性。不幸的是，这样的预期是巨大的。由于需要访问文件系统索引（FS 索引），<mark>顺序 I/O 实际上被分解为非顺序的小 I/O，如下所述。通常，FS 索引包括文件元数据（例如inode）</mark></p></div><div><p dir="auto">和资源分配映射（例如 bitmap），它们都存储在固定大小（例如 4KB）的文件系统块中</p></div><div><p dir="auto"></p><div src="读书笔记/曹老师2019年论文/assets/75e3a9552495f464139043818d995b59_md5.png" class="internal-embed media-embed image-embed is-loaded"><img src="读书笔记/曹老师2019年论文/assets/75e3a9552495f464139043818d995b59_md5.png"></div><p dir="auto"></p></div><div><p dir="auto">图 2：LSM 树的 LevelDB 实现。inmemory 组件&nbsp;C0C0​&nbsp;，由两个排序的 skiplist（MemTable 和 ImmTable）以及每个 on-disk component&nbsp;′C1′′C1′​&nbsp;C2C2​&nbsp;等组成。是 level&nbsp;(L0(L0​&nbsp;L1L1​&nbsp;等）。在整个运行时中，有三种类型的磁盘写入，如 fgure 所示：用于备份内存表的预写日志（步骤 1）;在压缩中写入新生成的块 （SST）（步骤 4）;并在压缩完成后更新 LSM 索引（步骤 5）。</p></div><div><p dir="auto">作为文件数据的组成部分。<mark>因此，对 LSM 块或 LSM 索引的更新通过非顺序的小写入来更新 FS 索引</mark>，从而导致不同程度的 I/O 放大，即 I/O 的数量和强度增加，这些 I/O 通常很小且不连续，具体取决于给定文件系统的性质。首先，对于就地更新文件系统（如 ext4），使用</p></div><div><p dir="auto">文件的元数据通常被积极地存储在一个地方，而资源分配映射存储在另一个地方，而用户数据存储在有足够可用空间的其他地方，无论在 ext4 中的组“块组”内或之外）。<mark>写入文件的数据块不可避免地会导致更新 FS 索引的 I/O。</mark>崩溃中的失败写入会导致文件系统处于不一致的状态 [53]，从而导致空间泄漏或文件损坏。为了保持数据一致性，就地更新文件系统通常使用日志来获取 inode 和 bitmap 的原子更新，这会带来额外的开销。也就是说，编写日志成为通常的 FS 索引更新不可或缺的一部分，并且是通常的 FS 索引更新的补充。因此，我们也将日志视为此类文件系统的 FS 索引。由于用户数据块和索引块存储在不同的地方，就地更新的放大 I/O 通常是对存储设备的随机访问其次，对于基于日志的文件系统 [56]，<mark>一个就地更新，尽管对 FS 索引的更新可以与用户数据相邻，但它必须更新从根 inode 到用户数据以及分配映射的整个元数据路径</mark>，这种现象称为 wanderingtree 更新 [33， 55， 56]。此外，必须采取措施回收就地更新文件系统的死数据块，这一过程称为垃圾回收 （GC）。如何实现高效的 GC 对于实际的日志文件系统至关重要，尤其是在压缩过程中产生大量垃圾的 LSM 树环境中。我们已经在 NILFS2 [30] 上运行 LSM 树进行了实验，NILFS2 [30] 是Linux 中，发现在写入的数据量仅为文件系统卷的十分之一后，文件系统会停止工作，因为没有剩余空间。<mark>F2fs [33] 是一种实用的日志结构文件系统，它通过为元数据块引入节点地址表 （NAT） 并以就地更新方式存储资源分配映射来解决游荡树更新问题，但代价是失去了日志记录功能的好处并导致更多的随机 I/O</mark>。<mark>日志文件系统的一个问题是，集群用户数据（例如 LSM 树日志）可以在存储空间中碎片化 [70]。第三种类型的文件系统是写入时复制 （CoW）</mark></p></div><div><p dir="auto">文件系统，另一个就地更新的实例，例如 btrfs [55]，它也以<mark>浮动方式</mark>更新索引，只是它们不保证更新在物理上是连续的。在这两种类型的 out-of-place-update 文件系统中，如FS 索引在每次更新中都会写入新位置，必须定期更新确定位置的锚点以记录 FS 索引的最新位置，以免丢失对更新数据的跟踪。<mark>对锚点的每次更新都代表文件系统的新版本。</mark>如果成功更新文件数据但未检查版本，则如果发生崩溃，则可能导致实际的文件更新失败。因此，我们也将锚点视为此类文件系统的 FS 索引的组成部分。如上一小节所述，LSM 树具有它自己的索引来定位和描述数据块。在文件系统上运行时，LSM 索引和 LSM 块都由底层文件系统组织成文件。对 LSM 树块的单次更新实际上需要多个 pbysical udaseborit I lse OSthefolowingt wotyes （I） I/O for a 4MB chunk）;（2） 更新 FS 索引的 FS 块（至少 2 个 I/O，具体取决于文件系统组织）。在更新 LSM 索引时，将重复相同的过程，例如：（1） 更新 LSM 索引的 FS 块;（2） FS 索引的 FS 块更新。图 3 分别说明了 LSM 树通过三个代表性文件系统和 LDS 的写入模式。它表明 I/O 的数量显著放大，因为 LSM 树中预期的大型顺序写入 I/O 实际上通过文件系统在存储设备上转换为大量小型且可能非顺序的写入 I/O。为了提供更多见解，图 4a 显示了</p></div><div><p dir="auto">当 LSM 树通过三个代表性文件系统运行时，总 I/O 的分数实际上是 FS 索引 I/O。我们还在图 4b 中显示了在原始 HDD 和 SSD 设备上持久保存不同大小的请求的 I/O 延迟。在图 4a 的 oun 实验中，我们通过分析在禁用备份日志的情况下顺序写入 LevelDB 的块跟踪结果来识别 FS 索引 I/O，以便仅持久更新块文件和 MANIFEST 文件。对于图 4b，每个结果是通过按顺序向原始设备发送一系列 write-fsync 请求对 10 秒来获得的，并使用请求的平均响应时间</p></div><div><hr></div><div><p dir="auto"></p><div src="读书笔记/曹老师2019年论文/assets/086701fda35a7ff44e88bc56ed71ac6c_md5.png" class="internal-embed media-embed image-embed is-loaded"><img src="读书笔记/曹老师2019年论文/assets/086701fda35a7ff44e88bc56ed71ac6c_md5.png"></div><p dir="auto"></p></div><div><p dir="auto">图 3：图 a、b 和 c 显示了 LSM 树通过三个代表性文件系统的写入模式，其中大写入是对 LSM 块的更新，而小写入是对 FS 索引和 LSM 索引的更新。图 d 显示 FS 索引写入被 LDS 完全消除，留下 LSM 索引写入。此图的实验是在 sequential workloads 下禁用备份日志的情况下运行的。</p></div><div><p dir="auto">由于两级索引（LSM 索引和 FS 索引）导致的 I/O 这种差异不仅对 LSM 树的性能产生不利影响，而且还会损害底层存储设备（例如 SSD）在压力大的 LSM 树环境中的性能和耐用性文件系统的好处之一是它对对象的支持</p></div><div><p dir="auto"></p><div src="读书笔记/曹老师2019年论文/assets/2afecc8cdef7da02a715c0651dd9acda_md5.png" class="internal-embed media-embed image-embed is-loaded"><img src="读书笔记/曹老师2019年论文/assets/2afecc8cdef7da02a715c0651dd9acda_md5.png"></div><p dir="auto"></p></div><div><p dir="auto">图 4：（a） 通过三种代表性类型的文件系统存储 LSM 树时的 I/O 分布。（b）&nbsp;I/0I/0&nbsp;在 HDD 和 SSD 上持久保存不同大小的请求的延迟。</p></div><div><p dir="auto">（文件），这种抽象使数据库存储能够提供它想要的任何更高级别的对象 [62]。但是，随着键值存储的普及和 LSM 树的广泛部署，来自应用程序的统一数据对象从文件系统抽象中受益甚微。因此，有必要了解存储堆栈引起的开销，以便开发高性能和可靠的 LSM 树存储系统。在本文中，我们研究了不同文件系统在存储 LSM 树数据时的行为，<mark>并设计了一个使用 LSM 索引直接管理存储空间并保留 LSM 树所需属性的系统</mark>。鉴于键值存储一直充当关系数据库（如 MySQL）[65]、分布式存储（如 MongoDB）或旨在加速小写入的文件系统 [15， 25， 54] 的存储引擎，我们提出的 LSM 树直接管理存储 （LDS） 为存储系统的设计提供了新的机会，以实现性能和一致性维护的显着改进。 本文的其余部分将详细介绍</p></div><div><p dir="auto">作为延迟测量。实验结果清楚地表明，文件系统诱导的 inder I/O 虽然体积小，但远远超过实际用户数据 I/O 的数量，并大大降低了 LSM 树应用程序的性能。虽然大型 SST 产生的 FS 索引开销较低，但它们有限制。</p></div><div><p dir="auto">首先，考虑到键值对本应很小 [2， 45， 46， 69] 这一事实，较大的 ==SST ==总是会导致更大的索引数据，这会导致更长的搜索路径。相反，较小的 SST 可以很好地组织起来，以将索引数据放入一个存储块中，以实现高效搜索。其次，大型 SST 上的压缩需要预留足够的可用存储空间来容纳新的 SST [20]，并且需要很长时间才能完成合并过程，这可能会阻止更紧急的操作，从长远来看会导致性能抖动和降级 [17]。在本文中，我们重点介绍了 LevelDB 采用的 SST 设置（例如 2~4MB），并提供了一个高性能的存储系统，该系统利用 LSM 树结构来完全消除 FS 索引</p></div><div class="heading-wrapper"><h3 data-heading="3设计与实施" dir="auto" class="heading" id="3设计与实施"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>3设计与实施</h3><div class="heading-children"><div><p dir="auto"><mark>LDS 的目的是消除额外的 FS 索引，并在 LSM 树数据与其物理位置之间执行直接映射。</mark>我们通过设计一个 LSM 树友好的磁盘布局，并明确地将<mark>日志存储与 chunk 存储分开</mark>来实现这一点。在本节中，我们首先介绍 LSM 树自定义磁盘布局，以及 LSM 树数据是如何分类的。然后我们描述了 LDS 中管理存储管理的原则。本节最后讨论了重要的实施问题</p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="2.3Motivation" dir="auto" class="heading" id="2.3Motivation"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>2.3Motivation</h2><div class="heading-children"><div><p dir="auto">上述对文件系统上运行的 LSM 树的分析表明，LSM 树的预期顺序聚合写入 I/O 与生成的文件系统非顺序（随机）小型写入之间存在明显的脱节和差异</p></div><div><hr></div><div><p dir="auto"></p><div src="读书笔记/曹老师2019年论文/assets/0016bfccfd30573c81a873ba3dcd33aa_md5.png" class="internal-embed media-embed image-embed is-loaded"><img src="读书笔记/曹老师2019年论文/assets/0016bfccfd30573c81a873ba3dcd33aa_md5.png"></div><p dir="auto"></p></div><div><p dir="auto">图 5：LSM 树数据的 LDS 在磁盘上的布局</p></div><div><p dir="auto"></p><div src="读书笔记/曹老师2019年论文/assets/f87b5943ad93d3bb43c38ddd6e856385_md5.png" class="internal-embed media-embed image-embed is-loaded"><img src="读书笔记/曹老师2019年论文/assets/f87b5943ad93d3bb43c38ddd6e856385_md5.png"></div><p dir="auto"></p></div><div><p dir="auto">Figure 6: Log object format.</p></div></div></div><div class="heading-wrapper"><h2 data-heading="3.1磁盘布局" dir="auto" class="heading" id="3.1磁盘布局"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>&nbsp;3.1磁盘布局</h2><div class="heading-children"><div><p dir="auto">LDS 的磁盘布局如图 5 所示。整个卷分为三个区域：版本日志区域、备份日志区域和插槽区域。插槽区域中的所有插槽具有相同的大小，并按它们到第一个插槽的偏移量（插槽 ID）按顺序编号。插槽可以包含一个 LSM 树块，该块也具有从偏移量派生的 ID（块 ID），因此 LDS 可以立即定位给定块的物理位置。这两个日志区域包含具有 Figure 6 所示合法格式的连续日志对象，其中 magic 和&nbsp;CRCCRC&nbsp;fields 用于确保日志对象的完整性，type 和&nbsp;SNSN&nbsp;（sequence number） 字段用于标识恢复中的活动日志对象（在 Section 3.3 中描述），size 字段表示有效负载包含多少字节。活动对象是在恢复中应考虑的对象。相反的是应该忽略的过时对象：版本日志包含两种主要类型的对象。(1)</p></div><div><p dir="auto">基本版本 （BV） 包含生成基本版本时 LSM 树的完整描述：所有块的元数据等。数据块的元数据由数据块 ID、它所属的级别、数据块的最小和最大键组成。此外，LDS 会复制基本版本中的存储格式信息。（2） △ 版本 （△V） 描述了压缩的结果，例如，应该删除的过时 chunk 和应该添加的新 chunk。所有版本及其相应的基本版本都可以合并到新的基本版本，此过程称为 trim。备份日志主要包含 Write-Ahead-Log （WAL） 对象，这些对象为尚未持久化到磁盘结构中的内存中键值对（即图 2 中的 MemTable 和 ImmTable）提供备份。slot 区域存储 LSM 树块，每个</p></div><div><p dir="auto">一个区块。如前所述，块由一个包含已排序键值对的 body 和一个 tail 部分组成，用于在正文中为请求的键编制索引。由于 LSM 树不能总是生成具有其托管槽确切长度的块，因此块的主体和尾部可能不会完全占据槽。一个小的填充区域用于使尾部与其槽边界右对齐，如图 7 所示，因此右对齐</p></div><div><p dir="auto"></p><div src="读书笔记/曹老师2019年论文/assets/6763d162069f4b0422e9ef79456c4bb8_md5.png" class="internal-embed media-embed image-embed is-loaded"><img src="读书笔记/曹老师2019年论文/assets/6763d162069f4b0422e9ef79456c4bb8_md5.png"></div><p dir="auto"></p></div><div><p dir="auto">图 7：Chunk 包。Padding 用于填充插槽左侧的 body 部分和 Tail 右侧的 tail 部分之间的间隙。</p></div><div><p dir="auto">尾巴的末端可以立即找到 2.此存储解决方案不会导致存储空间中出现外部碎片，删除块会立即释放其托管槽以供重新分配。因此，LDS 不存在一般 CoW 系统的垃圾收集和碎片整理问题 [33， 55]。但是，slot 内的填充区域会导致内部碎片化，这将在 Section 3.4 中讨论。</p></div></div></div><div class="heading-wrapper"><h2 data-heading="3.2LSM-tree Managed Storage" dir="auto" class="heading" id="3.2LSM-tree_Managed_Storage"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>3.2LSM-tree Managed Storage</h2><div class="heading-children"><div><p dir="auto">一个完整的 LSM 树包括版本（LSM 树索引）、备份日志（内存中 LSM-trec 组件的磁盘备份）和 chunks（自索引键值组）。在本节中，我们将介绍如何保持这三个区域的写入一致性。版本表示备份状态的快照，</p></div><div><p dir="auto">块组织。如果从关卡中删除任何数据块或将其插入到关卡中，则可以在将来更改组织。这只发生在压缩过程中。压缩完成后，应从版本中删除参与压缩的块的元数据，并将从压缩生成的块的元数据添加到版本中。回想一下，块从一个级别到另一个级别的微不足道的移动也被视为压缩操作，包括内存表转储。如果内存表被压缩到磁盘块，则备份日志中活动对象的起点将被重置。新的起点记录在表示此内存压缩的版本中。LSM 树不会就地更新版本，而是通过在版本日志中附加更改 （△version） 来提交更改 （△version），并将一组版本与基本版本合并以生成新的基本版本。只有成功提交的 △ 版本才能保证压缩结果在 LSM 树上。否则，压缩的任何结果都将被丢弃，就像什么都没发生一样。由于新 chunk 始终写入空闲插槽中，并且在提交 Aversion 之前不会被其他压缩看到，因此损坏的压缩对原始数据没有影响。对于内存压缩，由于备份日志的重置起点记录在 △ 版本中，因此 Aversion 失败只会取消该重置。换句话说，提交 Aversion （1） 会从版本中删除旧块，（2） 将新块添加到版本，以及 （3） 以原子方式重置备份日志的起点。</p></div><div><hr></div><div><p dir="auto"></p><div src="读书笔记/曹老师2019年论文/assets/d926e0a66b683f833b7b9366d5fbb352_md5.png" class="internal-embed media-embed image-embed is-loaded"><img src="读书笔记/曹老师2019年论文/assets/d926e0a66b683f833b7b9366d5fbb352_md5.png"></div><p dir="auto"></p></div><div><p dir="auto">图 8：修剪以生成新的基本版本。这些版本与其基本版本共享相同的 SNvalue</p></div><div><p dir="auto">插槽使用状态（已分配或可用）可以通过检查基本版本和 Aversion.重新启动时，恢复过程将在线构建一个位图，用于在执行修剪时跟踪插槽使用情况。我们将此位图称为 online-map，以区别于单独存储在持久化存储中的传统位图，必须将其视为一致性控制。在运行时分配槽会立即在 online-map 中 FIF 其状态（从 free 到 allocated），以防止再次分配它们。但是，如果发生崩溃，只有成功提交的 △ 版本才会保持翻转结果。必须在提交 Aversion 后执行将插槽的状态从 allocated 翻转为 free 以删除 chunk;否则，数据可能会损坏，因为 LSM 树可能会恢复到已提交的先前版本，但已重新分配槽以存储错误的数据。例如，考虑到在 Aversion r 中分配并在 Aversion&nbsp;yy&nbsp;中释放的插槽，如果在提交 △version&nbsp;yy&nbsp;之前，该插槽在在线地图中的状态翻转为 free，则可能会在发生崩溃时分配该插槽来存储新的块。因此，在重新启动时，系统会将插槽恢复到 Aversion&nbsp;XLXL&nbsp;，但插槽存储了错误的数据。当崩溃发生时，在线映射中未释放的插槽永远不会导致空间泄漏，因为恢复中的 trim 过程将根据提交的版本构建在线映射如果压缩生成新的块，LDS 必须分配</p></div><div><p dir="auto">free 插槽来持久化这些块。分配可以通过基于在线地图的任何方式实现。新的 chunk 将根据其分配的 slot 的偏移量分配一个 ID，这样从版本中记录的 chunk ID 中我们可以直接知道 chunk 的托管 slot。LDS 中的默认分配实现类似于 LFS 中的线程日志记录 [33， 56]，但以插槽作为主要单元。也就是说，LDS 在扫描在线地图以寻找空闲时段时总是朝一个方向前进，并在到达终点时绕行。如果一轮完整的扫描未能找到足够的空闲槽，则会报告“space full”状态，尽管联机映射是纯内存中数据结构</p></div><div><p dir="auto">非常小的大小（例如，对于具有 4MB 插槽的 100GB 存储，只需要 3.2KB 内存），当空间接近满时，分配过程可能会变得效率低下，因为它需要在在线映射中扫描更多位才能找到空闲位。为了加快分配过程，LDS 还维护了一小部分将在不久的将来分配的空闲插槽，称为 partial-list。当 find_free 线程的大小低于阈值时，将在后台触发该线程，以将空闲插槽 ID 附加到部分列表。</p></div></div></div><div class="heading-wrapper"><h2 data-heading="3.3Log Write" dir="auto" class="heading" id="3.3Log_Write"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>3.3Log Write</h2><div class="heading-children"><div><p dir="auto">日志是 LSM 树中的一个重要组成部分。除了支持内存表的备份日志外，版本还以日志记录方式更新（即 Aversion）。在本节中，我们将介绍 LDS 中的高效日志机制，以及如何从日志崩溃中恢复一致状态。</p></div><div><p dir="auto">3.3.1严格附加和崩溃恢复。版本日志和备份日志都以循环方式使用，这是日志记录或日记的常见用法。通过文件系统存储的日志通常在追加操作后必须更新文件索引。对于采用日志的文件系统本身，例如，ext4 中的日志，在日志区域的开头设置一个超级块（journal super）[53]，并在之后更新以识别实时和过时的日志项。相比之下，LDS 仅使用一个物理追加操作来更新日志，而无需更新任何其他标识数据。LDS 通过使用 log 对象中的一些特殊字段（例如 type 和 SN，图 6）来识别活动对象来实现这一点。对于版本日志，最新的基本版本用作</p></div><div><p dir="auto">分离活动对象和过时对象，如图 8 所示。每次执行 trim 过程时，都会保留基本版本。现在，我们假设 version 区域以 legal 对象开头，稍后我们将讨论如何识别日志是否回绕。在恢复过程中，LDS 会从头开始扫描版本日志区域中的所有对象，直到遇到垃圾（即格式非法或 SN 值小于最新扫描的 SN 值），并识别最新的 Base 版本（SN 值最大）及其所有具有相同 SN 值的后续版本，以恢复版本结构和在线地图。对于备份日志，如 Section 3.2 中所述，活动对象的起点可以从对应于最新内存压缩的 Aversion 中获得。恢复内存表是通过从起点扫描备份日志来实现的。</p></div><div><p dir="auto">当版本日志和备份日志末尾的剩余空间不足以容纳所请求的对象时，版本日志和备份日志都会自动换行。为了使 wrapping 状态对恢复过程可识别，LDS 引入了两个特殊对象。如图 9 所示，一个在日志换行时附加到最后一个对象旁边，称为右侧的 urapping 对象 （WOR），另一个在日志换行时放置在日志区域的开头，称为左侧的包装对象 （WOL）。当日志回绕时，WOL 包括一个指向第一个活动对象的指针，而 WOR 只是一个边界标识符，通知恢复过程返回到日志区域的开头。在实践中，只有版本日志 WOL 生效，因为 LDS 总是从日志区域的开头扫描对象，并且只需要跟踪右端活动对象，而对于备份日志，只有 WOR 生效，因为 Aversion 已经指定了起点，但如果日志已经包装，它需要知道包装边界。然而，在扫描版本日志时，LDS 不会立即转到 WOL 指向的右侧部分，因为它们可能已经过时（通过修剪过程）。由于包装对象与右端对象具有相同的 SN</p></div><div><hr></div><div><p dir="auto">耐用。LDS 保证在收到同步请求时，操作系统缓存的插入将提交到备份日志中上述提交策略提出了一个问题，即如何</p></div><div><p dir="auto">标识恢复中备份日志的起点，因为 Aversion 指向的起点对象可能尚未保留。为了解决此问题，LDS 还会在版本中记录 start point 对象的 SN。如果恢复过程发现起始点对象的 SN 与 △version 中记录的 SN 匹配，则直接忽略备份日志。</p></div><div><p dir="auto"></p><div src="读书笔记/曹老师2019年论文/assets/a58d48af663aed636c8c5460886af70c_md5.png" class="internal-embed media-embed image-embed is-loaded"><img src="读书笔记/曹老师2019年论文/assets/a58d48af663aed636c8c5460886af70c_md5.png"></div><p dir="auto"></p></div><div class="heading-wrapper"><h3 data-heading="图 9：左侧的包装对象 （WoL） 和右侧的包装对象 （WOR）。" dir="auto" class="heading" id="图_9：左侧的包装对象_（WoL）_和右侧的包装对象_（WOR）。"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>图 9：左侧的包装对象 （WoL） 和右侧的包装对象 （WOR）。</h3><div class="heading-children"><div><p dir="auto">当日志环绕时，LDS 通过查找 WOL 之后的基本版本来了解它们是否过时。如果 LDS 无法从左侧的合法对象中找到实时基础版本，则 LDS 会转向右侧部分：这意味着左侧的对象都是实时版本，并且它们的基本版本存在于右侧部分。由于日志区域通常不大，因此扫描</p></div><div><p dir="auto">原木区域不会像人们想象的那么昂贵。例如，对于配置了 4MB LSM 树块的 100GB 存储，2MB 版本日志通过定期修剪就足够了，考虑到对象头开销，16MB 备份日志对于两个内存表（图 2）就足够了。即使在运行时没有修剪，就像在 LevelDB 中一样，累积的版本数据也不会超过 50MB，持续的随机写入会填满存储空间。此外。将物理上连续的数据块加载到内存是利用底层存储设备的高级顺序带宽的最佳方式。例如，商用 HDD 的顺序读取带宽为 150MB/s，只需 0.33 秒即可将 50MB 版本日志加载到内存中，并且在内存中执行扫描速度很快。我们在第 4.4 节中评估了恢复成本。</p></div><div><p dir="auto">3.3.2提交策略。在 LDS 中，每个压缩结果（versio）都提交到版本日志中，以提供最新磁盘结构的一致状态，而 WriteAhead-Logs （WAL） 被提交到备份日志中，以便恢复内存中的键值对。崩溃前的未提交 Aversion 将使压缩过程已完成的所有工作无效，而延迟提交会阻止其他压缩操作对与此未提交 Aversion 相关的块进行操作。在 LevelDB 中，立即提交 Aversion：最近插入到备份的未提交 WAL</p></div><div><p dir="auto">log 将导致 Insertions 丢失。但是，提交每个 WAL 的成本非常高，因为 I/O 延迟比内存操作长几个数量级。低延迟和字节可寻址的 NVM 技术很有希望用于 WAL 提交 [29]，但尚未得到广泛使用。因此，用户必须在性能和持久性之间做出自己的权衡。例如，一些应用程序频繁地进行 fsync 调用来提交最近的写入，以确保高持久性 [26， 47]，但代价是吞吐量下降，而其他应用程序可能会通过牺牲持久性来禁用备份日志或将累积日志刷新到操作系统缓存以实现高吞吐量 [37， 57]。LDS 继承了 LevelDB 策略，该策略默认将每个 WAL 推送到 OS 缓存。如果用户希望插入请求</p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="3.4Internal Fragmentation" dir="auto" class="heading" id="3.4Internal_Fragmentation"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>3.4Internal Fragmentation</h2><div class="heading-children"><div><p dir="auto">块的主体部分和尾部部分之间的填充（图 7）会导致内部碎片化，从而导致 LDS 中的一些存储空间浪费。在压缩的 merge 排序中，merge 进程</p></div><div><p dir="auto">并行遍历多个 chunk 并在新 chunk 的主体中对它们的键值对进行排序。新 chunk 的尾部会随着 body 的增加而更新。在检查了主体和尾部的大小后，将块打包。如果合并过程发现向 body 再添加一个键值对会导致 package 溢出 slot 大小，则不会添加此键值对，而是轮流执行打包。在这种情况下，可能会发生内部碎片，因为插槽有一些可用空间，但不足以用于合并过程的下一个键值对。这种碎片也存在于文件系统中，因为很难生成与文件级块大小（例如 4KB）完全一致的块文件。只要键值对的大小小于文件级块的大小，这是 LSM 树键值存储中的常见情况 [2， 45， 46， 69]，LDS 中的内部碎片就不会比文件系统中的更有害，但是，在合并排序结束时，</p></div><div><p dir="auto">无论 last chunk 包含的数据多么少，都必须打包它。LDS 中的此类数据块称为具有可变大小的奇数数据块，关卡中存在的奇数数据块过多会导致严重的内部碎片。为了减少 odd chunk 引起的内部碎片，我们对每次 compaction 中 odd chunk 的处理逐级&nbsp;LL&nbsp;进行了小幅更改&nbsp;L+1L+1&nbsp;。奇数块不是放在 level&nbsp;L+1L+1&nbsp;中，而是保留在 level&nbsp;LL&nbsp;中，并且在未来的操作中有两种可能性。一种是它被 level&nbsp;L−1L−1&nbsp;的下一个压缩选择为重叠的 chunk。另一个是它附加到它的下一个相邻块，该块将参与 level&nbsp;LL&nbsp;的下一次压缩。在这两种情况下，奇数块都被同化和吸收。这样，每个级别最多有一个奇数块，而不管 store 的大小如何。由于奇数块不与关卡&nbsp;LL&nbsp;中的任何块重叠，因此&nbsp;L+1L+1&nbsp;将其放置在关卡&nbsp;LL&nbsp;中不会破坏树结构。另一种替代方法是优先选择奇数块作为压缩的受害者，而不是以循环方式选择</p></div><div><hr></div></div></div><div class="heading-wrapper"><h2 data-heading="3.5实现" dir="auto" class="heading" id="3.5实现"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>&nbsp;3.5实现</h2><div class="heading-children"><div><p dir="auto">我们基于 LevelDB 1.18 实现了一个 LDS 原型来评估我们的设计。</p></div><div><p dir="auto">第一个任务是管理存储空间，称为 I/O 层。我们使用 ioctl 获取存储分区的属性，并通过在版本日志中写入初始版本来启动空间。<mark>物理空间上的 I/O 操作由 open/write/mmap 系统调用实现</mark>。对于同步请求，调用 sync_file_range [42] 并设置了必要的标志？保证数据持久化虽然 LDS 可以掌控缓冲区管理工作，选择合适的 flush 机会，在不失去数据一致性的情况下实现更好的性能，比如在后台进程中启用并发合并和 flushing，但我们目前在原型中实现了这个功能，以与 LevelDB 中相同的方式工作， 从而提供公正的评估结果。一般来说，LDS 区分了 chunk、version log 和 backup log 三种类型的写入请求，并根据上述设计分别处理它们。总共编写了 456 行代码来实现 I/O 层。第二个任务是修改 LevelDB，使其可以在 LDS 的 I/O 层上运行，主要是 env 接口的新实现，总共编写了 32 行代码。</p></div></div></div><div class="heading-wrapper"><h2 data-heading="3.6 Scalability" dir="auto" class="heading" id="3.6_Scalability"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>3.6 Scalability</h2><div class="heading-children"><div><p dir="auto">3.6.1 使用预先分配的文件空间。LDS 可以用作存储引擎，<mark>通过提供键值接口 （put/get/delete） 来管理低级存储设备。</mark>尽管如此，在文件系统预先分配的文件空间上运行 LDS 是很容易可行的。对于用户来说，使用预先分配的文件空间与使用原始设备完全相同，但 LDS 内部不能直接在预先分配的文件空间上使用sync_file_range，因为 sync_file_range 的使用限制以及文件系统对物理空间分配的潜在干扰一般来说，预先分配的分配的文件空间从 O 开始，并静态映射到 LDS。分配文件空间的文件系统维护文件空间与 inode 中的存储空间之间的映射。<mark>sync_file_range 仅确保文件空间范围内的数据同步到相应的存储空间 [42]，但不确保 inode 数据（映射信息）同步。</mark>如果文件系统就地更新 [40]，则在文件空间中写入和同步数据时，映射信息不会更改，并且在崩溃后检索同步数据没有问题。但是，如果文件系统不就地更新[33， 5]，则在其文件空间中写入和同步始终会导致数据被保存在新的存储位置，并且应更新 inode 中的映射数据以跟踪新位置。在这种情况下，如果映射数据未同步，则文件空间中的同步数据将在崩溃后丢失。因此，fsync/fdatasync 必须用于通知文件系统同步映射信息预先分配的文件空间中的备份日志需要专门针对就地更新文件系统处理。如 Section 3.3.2 中所述，<mark>备份日志的同步速度通常与 slot 或 version 数据的速度不同。但是，预先分配的文件空间上的 fsync 适用于整个文件空间，在用户选择延迟提交备份日志的情况下，会导致性能下降</mark>。一个实用的解决方案是为备份日志指定一个单独的分配文件空间。<mark>实际上，将备份日志单独存储在不同的存储空间中是提高日志记录效率的实用方法 [12]，本文不再进一步详细说明。</mark></p></div><div><p dir="auto">3.6.2 存储大小调整。LDS 适用于原始设备空间和文件系统分配空间，通过在版本中设置特殊字段来描述其管理的存储空间，可以根据用户扩展或缩小存储空间的需求灵活调整空间</p></div><div><p dir="auto">扩展存储空间，即加入新设备或从文件系统请求更多空间，是通过重新构建<mark>在线地图</mark>来体现新空间的扩展槽来实现的。将生成一个新版本以包含已加入空间的信息。为了缩小存储空间，即删除设备或为文件系统返回一些空间，LDS 首先将缩小槽中的块复制到其他空闲槽，然后修剪版本并重新构建在线映射以排除缩小的槽。在修剪过程中。最初存储在 shrunk slot 中的 chunks 将根据其新的托管 slot 分配新的 ID。</p></div><div class="heading-wrapper"><h3 data-heading="4 EVALUATION" dir="auto" class="heading" id="4_EVALUATION"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>4 EVALUATION</h3><div class="heading-children"><div><p dir="auto">本节介绍了恶魔 STON STS 的好处的实验结果</p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="4.1Environment Setup" dir="auto" class="heading" id="4.1Environment_Setup"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>4.1Environment Setup</h2><div class="heading-children"><div><p dir="auto">实验是在配备两个 Intel（R）Xeon（R） CPU E5-2683 v3 @ 2.00 GHz 处理器和 32GB RAM 的机器上进行的。操作系统为 64 位 Linux 4.4。使用的 HDD Seagate ST2000DM001 的容量为 1.8TB，顺序写入速度为 152MB/s，使用的 SSD Intel SSD DC S3520 系列 2.5 英寸的容量为 480GB，顺序写入速度为 360MB/s。请注意，HDD 在外柱面上的速度略快。我们的实验为每个系统选择从 800GB HDD 开始的分区，以最大限度地减少对单个实验的硬件影响。驱动器的写入缓存被禁用，以确保数据得到安全存储我们将 LDS 的性能与 LevelDB 的性能进行比较</p></div><div><p dir="auto">（1.18） 运行在三个典型的文件系统上，ext4 （就地更新） [40]， f2fs （基于日志） [33] 和 btrfs （写时复制） [55] 所有文件系统都使用 noatime 选项挂载，以消除与我们的评估无关的潜在开销 在 LevelDB 中，块 （SST） 大小配置为 4MB LDS 中的版本日志和备份日志配置为</p></div><div><hr></div><div><p dir="auto">分别为 64MB 和 16MB，槽位大小为 4MB该配置不会触发版本日志上的 trim 过程，这是 LevelDB 中的做法。修剪过程的成本与恢复过程一起评估（第 4.4 节）。在所有实验中禁用 LevelDB 中的数据压缩</p></div></div></div><div class="heading-wrapper"><h2 data-heading="4.2Write Performance" dir="auto" class="heading" id="4.2Write_Performance"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>4.2Write Performance</h2><div class="heading-children"><div><p dir="auto">在本小节中，我们使用 LevelDB （db_bench） 中的默认基准测试来分别评估 LDS 和 LSM-on-FS 在顺序和随机工作负载下的插入性能。我们还评估了同步模式下的插入性能。平均键值对大小为 116 字节（即 16B 键，值范围为 1B 到 200B，分布均匀）</p></div><div><p dir="auto">4.2.1 顺序工作负载。图 10 显示了顺序工作负载下的性能（以运行时间表示）与插入次数的函数关系。从图中我们可以看到，LDS 在 HDD 和 SSD 上的性能都最好。为了进一步分析结果，我们通过检查不同类型的操作 / 事件对运行时间的贡献，仔细研究图 11 中的时间成本。LSM 树</p></div><div><p dir="auto"></p><div src="读书笔记/曹老师2019年论文/assets/b9d799c0c72f55bdaca5a282c5adc5b5_md5.png" class="internal-embed media-embed image-embed is-loaded"><img src="读书笔记/曹老师2019年论文/assets/b9d799c0c72f55bdaca5a282c5adc5b5_md5.png"></div><p dir="auto"></p></div><div><p dir="auto">图 10：顺序插入性能（越低越好）。</p></div><div><p dir="auto">具有用于预写日志 （Log） 和 MemTable 插入 （Mem） 的前台线程 （Front），并在 MemTable 转换为 ImmTable 时触发后台线程 （Back） 进行压缩。如果后台线程没有及时完成该过程，则前台操作会减慢 （Wait）。在顺序工作负载下，压缩中没有合并排序，块写入仅发生在将 ImmTable 转储到&nbsp;L0L0​&nbsp;时。磁盘级别的压缩是一个简单的移动操作，它只更新 LSM 索引。在 HDD 上，LSM-on-FS 中的后台进程很慢</p></div><div><p dir="auto">由于 LSM 的 FS 索引更新频繁</p></div><div><p dir="auto"></p><div src="读书笔记/曹老师2019年论文/assets/8e53fd82d1e3050d2c352624d3a754db_md5.png" class="internal-embed media-embed image-embed is-loaded"><img src="读书笔记/曹老师2019年论文/assets/8e53fd82d1e3050d2c352624d3a754db_md5.png"></div><p dir="auto"></p></div><div><p dir="auto">图 1l：顺序插入的运行时间细分。</p></div><div><p dir="auto">chunk 和 LSM 索引，并且前台有等待时间。在 SSD 上，由于 fash 的低延迟，所有系统都可以快速完成后台处理，因此前台成本（主要是日志记录成本）主导了整体性能。然而，不同的文件系统会产生相应的日志请求开销，因为它们有自己的写入系统调用（从 LevelDB 的刷新转换而来）的处理机制，这些调用将 WAL 推送到操作系统缓存。例如，他们将检查是否有足够的空闲块用于写入，以保证未来的刷新不会失败 [68]。</p></div><div><p dir="auto">4.2.2 随机工作负载。随机工作负载下的性能结果如图 12 所示，以运行时间作为插入次数的函数。随机插入会导致后台频繁的 Compaction 合并操作，并且需要很长时间来执行 Merge 排序和 chunk 写入。因此，前台进程大部分时间都在等待后台进程，而块写入效率最高的系统性能最好，如图 13 中随机插入的运行时成本分布所示。后台进程可能会阻止前台进程，因为 LSM 树的每个级别都有一个容量限制以及两个内存表，如 2.1 中介绍的那样。当内存表已满且空间&nbsp;L0L0​&nbsp;处于压力下时，前台进程必须减慢插入操作或等待后台压缩产生足够的空间在&nbsp;L0L0​&nbsp;4.2.3 同步插入中。</p></div><div><p dir="auto">在默认设置下使用备份日志运行，即仅将每个预写日志刷新到 OS 缓冲区。但是，用户有时希望在插入请求成功返回后，他们发出的插入是持久的。在这种情况下，我们使用 LevelDB 提供的同步模式来评估性能。在同步模式下，插入吞吐量完全由备份日志的写入效率决定，无论工作负载是顺序的还是随机的。以插入操作延迟测量的插入效率如图 14 所示。对于同步模式下的插入请求，LDS 可以</p></div><div><p dir="auto">实现相当于在原始存储设备上写入相同大小数据的效率（参见图 4b）。这是</p></div><div><hr></div><div><p dir="auto"></p><div src="读书笔记/曹老师2019年论文/assets/b3b2f034bcbf0fe0cc94bd6029e7ff30_md5.png" class="internal-embed media-embed image-embed is-loaded"><img src="读书笔记/曹老师2019年论文/assets/b3b2f034bcbf0fe0cc94bd6029e7ff30_md5.png"></div><p dir="auto"></p></div><div><p dir="auto">图 14：同步模式下的插入延迟。保证每个 WAL 都是持久的。</p></div><div><p dir="auto"></p><div src="读书笔记/曹老师2019年论文/assets/12191f4ba507070b53ec2980719a43d8_md5.png" class="internal-embed media-embed image-embed is-loaded"><img src="读书笔记/曹老师2019年论文/assets/12191f4ba507070b53ec2980719a43d8_md5.png"></div><p dir="auto"></p></div><div><p dir="auto">Figure 12: Random insertion performance (lower is better).</p></div><div><p dir="auto"></p><div src="读书笔记/曹老师2019年论文/assets/ccc5d72e20be61cf945f86a82a1fa490_md5.png" class="internal-embed media-embed image-embed is-loaded"><img src="读书笔记/曹老师2019年论文/assets/ccc5d72e20be61cf945f86a82a1fa490_md5.png"></div><p dir="auto"></p></div><div><p dir="auto">图 13：随机插入的运行时间明细。</p></div><div><p dir="auto">因为 LDS 在备份日志区域中只产生一个 I/O。在 LSM-on-FS 中，有几个 FS 索引块需要与备份文件更新一起更新，以保证请求在存储和文件系统中都持久存在。F2fs 通过实现前滚机制 [33] 针对小型同步请求进行了优化，该机制消除了许多 FS 索引更新，因此它的性能优于 ext4 和 btrfs。但是，f2fs 仍然需要为 FS 索引更新一个区块（即 f2fs 中的直接节点），这会导致比 LDS 更长的延迟。</p></div></div></div><div class="heading-wrapper"><h2 data-heading="4.3Read Performance" dir="auto" class="heading" id="4.3Read_Performance"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>4.3Read Performance</h2><div class="heading-children"><div><p dir="auto">我们加载 10 亿个固定大小（16B 键和 100B 值）的随机键值对来设置一个 100GB 的数据集来评估读取性能。可用的操作系统缓存限制为 1GB，以模拟&nbsp;100×100×&nbsp;X X 存储/内存配置。更大的存储系统可以具有更高的存储/内存配置比率 [63]。用于读取的并发线程数在 HDD 上设置为 4，在 SSD 上设置为 16。我们衡量</p></div><div><p dir="auto">冷缓存和暖缓存中的吞吐量和读取放大率，如图 15 所示。对于键的读取请求，它会在查找</p></div><div><p dir="auto">向上查找驻留在内存中的 LSM 索引。然后，不同的系统将 chunk ID 转换为 chunk 数据的磁盘位置。与 LSM-on-FS 必须读取 FS 索引（fle 元数据）才能定位 chunk 数据不同，LDS 可以直接从 LSM 索引中确定 chunk 位置。由于不同的系统以自己的方式组织存储空间和设计数据索引机制，它们会引起不同的读取放大，这不仅影响了缓存效率，还影响了读取性能。例如，ext4 将多个 inode 聚集在一个块中，而 f2fs 为每个节点对象专门分配一个块 [33]然而，有趣的是，btrfs 的性能特别低。对跟踪的进一步分析表明，btrfs 的读取流量明显高于其他 btrfs，如图 15c 所示。读取放大由平均 I/O 负责</p></div><div><p dir="auto">已处理请求的 traffic 中。在冷缓存中，几乎所有请求都使用 I/O 进行处理，以将可能包含请求的键值数据的存储块加载到内存缓存中，因此存在很高的读取放大率。随着缓存的预热，由于缓存命中而避免了一小部分 I/O，从而降低了读取放大率。特别是，对于单个读请求，btrfs 会在冷缓存中产生 8 个 I/O，其中一半大于 512KB，无论是 I/O 的数量还是大小，都远高于其他系统。由于我们使用相同的 mmap 系统调用从底层存储中读取 chunk 数据，因此读取放大的差异只能由不同系统的内部数据布局引起。Mohan 等人也观察到 btrfs 的高读长扩增 [44]</p></div></div></div></div></div><div class="heading-wrapper"><h1 data-heading="4.4恢复" dir="auto" class="heading" id="4.4恢复">&nbsp;4.4恢复</h1><div class="heading-children"><div><p dir="auto">在本小节中，我们将评估从磁盘版本日志中恢复内存中版本的成本。从备份日志中恢复内存表的过程与此类似，但无需扫描整个日志区域即可找到有效对象。我们对 LDS 的评估总是假设最坏的情况，也就是说，即使我们已经确定了所有活动对象，我们仍然扫描整个版本日志区域 （64MB）。只有在扫描完成后，我们才开始执行 TRIM。</p></div><div><hr></div><div><p dir="auto"></p><div src="读书笔记/曹老师2019年论文/assets/713c1428cb5466c1af016abe6e4d4d5f_md5.png" class="internal-embed media-embed image-embed is-loaded"><img src="读书笔记/曹老师2019年论文/assets/713c1428cb5466c1af016abe6e4d4d5f_md5.png"></div><p dir="auto"></p></div><div><p dir="auto">图 15：冷缓存和暖缓存的读取吞吐量和放大率。读取线程数在 HDD 上为 4，在 SSD 上为 16。第一个&nbsp;10×10×&nbsp;线程数请求将考虑冷缓存结果，而暖缓存的结果是在空闲缓存填满且吞吐量达到稳定状态后实现的。读取放大是通过请求的平均 IO 流量 （KB） 来衡量的。</p></div><div><p dir="auto">LevelDB 恢复过程的总时间成本用于衡量恢复性能。在文件系统准备好后执行 LSM-onFS 的实验，并且不考虑挂载期间的文件系统一致性检查 [38]。我们使用 4.2 节中的随机工作负载，插入 100~10 亿个键值对，生成不同大小的版本数据（从 3MB 到 47MB）</p></div><div><p dir="auto">data 和 2 秒，则恢复时间的差异归因于 I/O 成本。正如我们所知。日志结构的文件系统总是在块地址空间的 logging head 上为所有文件分配块。虽然版本文件（应用程序的逻辑日志）定期与 SST 文件写入混合附加，但它会被文件系统日志碎片化，这个问题类似于已知的 log-onlog 现象 [70]。因此，f2fs 所需的恢复时间明显长于 HDD 上的其他系统版本文件碎片化也可能发生在通用 CoW 文件系统 [50]（例如 btrfs）中。此外，如第 4.3 节所示，btrfs 具有很高的读取放大率，因此，在 HDD 和 SSD 上读取 btrfs 的版本文件都是一项昂贵的操作，当版本运行时，LDS 花费的时间比其他 LDS 略多。</p></div><div><p dir="auto">sion 数据很小。这是因为在最坏的情况下，LDS 必须加载整个日志区域并执行彻底扫描以查找活动对象，而不管版本数据大小如何，这在 HDD 上需要 0.35 秒，在 SSD 上需要 0.18 秒的恒定时间。通过这个小的权衡，LDS 可以在运行时执行高效的日志更新</p></div><div><p dir="auto"></p><div src="读书笔记/曹老师2019年论文/assets/bbff9ffde404abc4bab1a896611e3deb_md5.png" class="internal-embed media-embed image-embed is-loaded"><img src="读书笔记/曹老师2019年论文/assets/bbff9ffde404abc4bab1a896611e3deb_md5.png"></div><p dir="auto"></p></div><div class="heading-wrapper"><h2 data-heading="4.5Space Utilization" dir="auto" class="heading" id="4.5Space_Utilization"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>4.5Space Utilization</h2><div class="heading-children"><div><p dir="auto">图 16：恢复时间与 ac 累积版本数据大小的函数关系。</p></div><div><p dir="auto">在本小节中，我们比较了不同系统的空间利用率，以研究 LDS 中内部碎片化的影响。我们将利用率定义为系统可以在具有给定容量的存储设备上容纳的固定大小的键值对的数量。每个系统的实验都是通过使用随机工作负载用 116 字节的键值对（16B 键和 100B 值）填充 100GB 的存储设备来完成的，直到系统报告“空间已满”。为了检查 LDS 优化对减少奇数块诱导的碎片的有效性，我们还在没有这种优化的 LDS 上运行了一项测试（标记为 plain-LDS）空间利用率的比较如图 17 所示。从图中我们可以看到，在不采取任何措施减少碎片的情况下，普通 LDS 的利用率最低，</p></div><div><p dir="auto">图 16 显示了不同大小的累积版本数据的恢复时间。恢复成本主要来自加载版本数据的 I/O 成本和执行 trim 的 CPU 成本。虽然执行 trim 的过程对所有系统都是类似的，但花费的时间与版本数据大小成正比，对于 3MB 版本，需要 0.1 秒</p></div><div><hr></div><div><p dir="auto">容纳大约&nbsp;97%97%&nbsp;f2fs、&nbsp;93%93%&nbsp;ext4 和&nbsp;94%94%&nbsp;btrfs 的键值对数量。我们的调查表明，空间浪费主要来自奇数块。通过上述碎片减少优化，LDS 在所有系统中实现了最佳空间利用率。文件系统的低效主要来自 FS 索引引起的空间开销，这在 f2fs 中更为明显，因为它需要相当多的块来存储节点地址表。</p></div></div></div><div class="heading-wrapper"><h2 data-heading="4.6预先分配的文件空间开销" dir="auto" class="heading" id="4.6预先分配的文件空间开销"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>4.6预先分配的文件空间开销</h2><div class="heading-children"><div><p dir="auto">尽管 LDS 被设计为 LSM 树的原始设备空间管理器，但它很容易支持使用预分配文件空间的功能，但有一些限制，如 Section 3.6.1 中所述。在本小节中，我们进行了实验，以评估 LDS 使用来自三个代表性文件系统的预先分配的文件空间时的局限性，即文件系统干扰的影响。虽然 ext4 被称为就地更新文件系统，但其他两个是不合适的。更新文件系统。因此，我们继续在 ext4 分配的文件空间上使用 sync_file_range，并禁用 ext4 的日志，因为在写入 LSM 树数据时，inode 中的映射数据不会更新。在 f2fs 或 btrfs 预先分配的文件空间上，使用 fsync 来保证更新的映射数据与 LSM 树数据一起同步。评估结果表明，文件系统对预分配文件空间的干扰会引起两种开销。第一种是刷新</p></div><div><p dir="auto">在默认提交策略中从用户空间到 OS 缓存的 WAL，其中 LevelDB 的每个日志记录请求都转换为 urite 系统调用，并最终由分配文件空间的相应文件系统处理。这种开销与 LevelDB 直接在文件系统上运行时的开销几乎相同，如图 11 所示的日志成本。使用 mmap 实现 fushing 可以显着减少 WAL 开销，因为将日志发送到 OS 缓存将是一个 memcpy 操作，在建立日志区域的页表条目后不会导致系统调用。</p></div><div><p dir="auto"></p><div src="读书笔记/曹老师2019年论文/assets/671f80493a8e186f297aafcbc07613ad_md5.png" class="internal-embed media-embed image-embed is-loaded"><img src="读书笔记/曹老师2019年论文/assets/671f80493a8e186f297aafcbc07613ad_md5.png"></div><p dir="auto"></p></div><div><p dir="auto">图 17：当系统在 100GB 存储设备上报告“space full”时插入的键值对总数。</p></div><div><p dir="auto">另一种是由文件系统干扰引起的 I/O 开销，它主要存在于 out-of-placeupdate 文件系统上，因为它们总是为任何 LDS 写入分配新的存储块，并且需要在同步 LDS 数据时同步文件元数据。例如，在 HDD 上，使用 btrfs 分配的文件空间的开销大约&nbsp;2.5×2.5×&nbsp;高于使用原始空间，这相当于在 btrfs 上运行 LevelDB 的常规方式，因为无法避免 wandering-update。此值适用于&nbsp;1.4×1.4×&nbsp;f2fs 分配的文件空间，或在 f2fs 上运行 LevelDB 开销的一半，因为已为预先分配的文件空间建立了 NAT，并且大多数情况下，在同步 LDS 数据时需要同步一个间接节点 [33]。对于 ext4 分配的文件空间，虽然它不需要更新文件元数据，但开销相当于使用原始空间。例如，在预分配过程中，ext4 确定将属于预分配文件的所有物理块，并创建一个将文件空间（文件偏移量）映射到物理块空间（即 LBA）的 inode。由于文件系统就地更新，后续从 LDS 到文件空间的写入直接进入相应的物理块，并且仅同步文件空间中写入的数据就足以保证数据一致性，因为 inode 中的映射信息不会改变。因此，LDS 可以像处理原始空间一样处理 ext4 分配的文件空间。</p></div></div></div><div class="heading-wrapper"><h2 data-heading="5相关工作" dir="auto" class="heading" id="5相关工作"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>&nbsp;5相关工作</h2><div class="heading-children"></div></div><div class="heading-wrapper"><h2 data-heading="5.1写入优化的数据结构" dir="auto" class="heading" id="5.1写入优化的数据结构"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>5.1写入优化的数据结构</h2><div class="heading-children"><div><p dir="auto">SQL Server [64] 等传统数据库系统使用 B+trees 作为后端结构，这对于读取来说非常出色，但在写入方面性能很差。分形树 [46] 是作为 LSM 树的写入优化数据结构，它维护一个全局 B+树，每个节点都有一个缓冲区。更新通过中间节点的缓冲区批量下降到 B + 树的叶节点，这与 LSM 树提出的想法类似 [46]。写入优化的数据结构已被广泛用作现代数据存储中的存储引擎 [7， 19， 54， 61， 65， 66]。本文重点介绍优化基于 LSM 树的键值存储的存储堆栈</p></div></div></div><div class="heading-wrapper"><h2 data-heading="5.2Optimizations on LSM-trees" dir="auto" class="heading" id="5.2Optimizations_on_LSM-trees"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>5.2Optimizations on LSM-trees</h2><div class="heading-children"><div><p dir="auto">随着 LSM 树在大型数据存储中的普及，已经研究了大量技术来优化 LSM 树的写入效率。大多数工作都有助于减少写入放大。VT 树 [59] 通过仅合并块的重叠部分来优化高度顺序工作负载中的写入放大。Wisckey [37] 通过将值从 LSM 树中移出到单独的日志中来减少值诱导的放大，类似于 Bitcask [58] 中实现的方法，即使用内存中哈希表为值日志编制索引。LSM-trie [69] 使用键的哈希前缀来索引级别，通过允许级别内的重叠块，它显着减少了写入放大，如 Cassandra [1] 中的大小分层压缩策略或阶梯合并</p></div><div><hr></div><div><p dir="auto">机制 [24] 可以。TRIAD [3] 通过利用倾斜的工作负载和延迟压缩过程来优化写入放大。PebblesDB [52] 引入了碎片化 LSM 树 （FLSM） 机制，该机制允许级别内的重叠块以避免数据重写，并为用户提供可调整的参数，以便在写入 I/O 和读取延迟之间进行权衡。</p></div><div><p dir="auto">我们的工作与上述现有工作不同且正交，因为我们通过提供 LSM 树友好的磁盘数据布局来优化 LSM 树。</p></div></div></div><div class="heading-wrapper"><h2 data-heading="5.3绕过存储堆栈层" dir="auto" class="heading" id="5.3绕过存储堆栈层"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>5.3绕过存储堆栈层</h2><div class="heading-children"><div><p dir="auto">在数据库字段初期，数据直接存储在容量较小的块存储上，应用程序负责块 / 段分配和数据一致性 [36]。该文件系统旨在通过使用统一的文件级块组织存储空间并在数据对象和底层存储空间之间引入间接映射来提供任意大小对象的目录层次结构抽象和数据存储 [41， 62]。Stonebraker [62] 研究了由不同的操作系统组件（包括文件系统）引起的数据库系统开销。Engler 和 Kaashoek [14] 提议完全消除操作系统抽象，并允许应用程序从硬件中选择高效的实现。不过。在接下来的几十年里，随着存储容量的快速增长，与多个应用程序共享一个存储并将复杂的存储管理工作转移到文件系统是有利可图的。然而，随着大数据的出现，例如大规模的键值存储 [7， 19， 69]，数据大小很容易超出存储容量，而负责大型和统一数据对象的应用程序从文件系统层中受益匪浅。相比之下，由于额外的间接和一致性实施，文件系统可能会对高性能数据存储产生负面影响例如，最近关于键值存储的工作仍然存在</p></div><div><p dir="auto">由于观察到的性能下降，它们的数据绕过了文件系统 [48]。Papagiannis 等人提出了系统 Iris [49]，以减少低延迟存储设备的 I/O 路径中明显的软件开销。NVMKV [39] 是一种键值存储，它通过将每个单独的键值对直接哈希到 SSD 的稀疏 FTL 空间中，迈出了根本性的一步。然而，他们并没有明确量化不同文件系统的数据部署造成的开销，LSM 树应用程序如何受到影响仍不清楚。在本文中，对间接费用进行了深入研究</p></div><div><p dir="auto">由于通过代表性文件系统存储 LSM 树，我们设计了 LDS 以 LSM 树结构直接管理存储，以提供高性能的键值存储。</p></div></div></div><div class="heading-wrapper"><h2 data-heading="5.4New Storage Technology" dir="auto" class="heading" id="5.4New_Storage_Technology"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>5.4New Storage Technology</h2><div class="heading-children"><div><p dir="auto">新技术（如多流 NVM）已被预先用于感知应用层数据流 [27]，这可能是 LDS 存储 LSM 树的机会</p></div><div><p dir="auto">data 以 NVM 友好的方式。FlashBlade [63] 构建了一个基于 flash 的存储阵列，该阵列将 flash 转换功能移动到阵列级软件中，并要求软件仔细地将用户数据调节为顺序流。LDS 提供了一种在应用层管理 flash 转换功能的简单方法，因为它消除了存储堆栈中的额外 I/O 并保留了 LSM 树的连续 I/O 模式。其他一些工作利用了新存储的特性</p></div><div><p dir="auto">来自应用程序层的媒体。LOCS [67] 通过将 SSD 的通道暴露给上层应用程序来挖掘 SSD 的带宽利用率，从而优化了 LSM 树应用程序的性能。Lee 等人 [34] 提出了一个应用程序管理的 flash 系统，它解决了应用程序层日志记录和 flash 层日志记录之间的差异，从而提高了应用程序性能和 fash 管理开销。Colgrove 等人 [10] 引入了一种存储系统，它使用自定义内核模块绕过内核块设备，并将应用程序级随机写入转换为压缩的顺序写入，以使底层闪存阵列受益 [63]。虽然 LDS 为基于 LSM 树的 LSM 提供了高性能</p></div><div><p dir="auto">键值存储，如果考虑到 SSD 的内部特性，则可以从 LDS 中获得潜在的好处。例如，可以消除闪存中昂贵的垃圾回收操作，因为 LDS 总是丢弃插槽单元中的存储空间，无需数据迁移即可擦除。此外，我们计划将 LDS 增强为闪存感知，以便它可以执行磨损均衡工作，这对 LDS 来说更简单、更方便。</p></div><div class="heading-wrapper"><h3 data-heading="6CONCLUSION" dir="auto" class="heading" id="6CONCLUSION"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>6CONCLUSION</h3><div class="heading-children"><div><p dir="auto">在本文中，我们介绍了 LDS，这是一种基于 Log-structured-merge-trec 的直接存储系统，它采用 LSM 树结构（一种广泛用于大规模键值存储的结构）来管理底层存储空间，从而保留 LSM 树的全部属性。基于 LevelDB 的 LDS 原型表明，与在最先进的文件系统上运行的 LSM 树相比，LDS 提供了显著的性能改进和 I/O 减少。</p></div></div></div><div class="heading-wrapper"><h3 data-heading="ACKNOWLEDGMENTS" dir="auto" class="heading" id="ACKNOWLEDGMENTS"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>ACKNOWLEDGMENTS</h3><div class="heading-children"><div><p dir="auto">我们感谢我们的牧羊人 Vijay Chidambaram 和 Russell Sears 为改进本文提供的大量帮助，并感谢匿名审稿人的宝贵意见我们还感谢 Mark Callaghan 的有益反馈。这项工作部分得到了武汉光电子国家实验室基金（资助号0106187015和0106187027）和美国国家科学基金会（资助号：CCF 1629625</p></div></div></div><div class="heading-wrapper"><h3 data-heading="引用" dir="auto" class="heading" id="引用"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>&nbsp;引用</h3><div class="heading-children"><div><p dir="auto">[1] 阿帕奇。2016. 压缩类型。http：/ /cassandra.apache org/doc/latest/operating/compaction.html.(2016)</p></div><div><hr></div><div><p dir="auto">[2Bek All YaWn rc SJan Value Store.在 ACM SIGMETRICS 性能评估评论中，第 40 卷.ACM，5364。[3]Oana Balmau、Diego Didona、Rachid Guerraoui、Willy Zwaenepoel、Huapeng Yuan、Aashray Arora、Karan Gupta 和 Pavan Konka。2017. TRIAD：在日志结构化键值存储中的 Mem、Disk 和 Log 之间创建协同作用。2017 年 USENIX 年度技术会议 （USENIX ATC 17）。[4]Michael A Bender、Martin Farach-Colton、Jeremy T Fineman、Yonatan R Fogel、Bradley C Kuszmaul 和 Jelani Nelson。20077 Cache-Oblivious Streaming B 树。在第九届年度 ACM 研讨会的会议记录中，关于并行算法和 ar chifecfures。ACM， 8192 [5]Gerth Stolting Brodal 和 Rolf Fagerberg.2003. 外部存储器词典的下限。第 14 届年度 ACM-SIAM 离散算法研讨会论文集。工业与应用数学学会，546554。[ Ad Graph 遍历。在 SODA 中。859860 [7]EChefBmt H Fikes.and Robert E Gruber.2008.Bigtable： A Distributed Storage System for Structured Data.ACM Transactions on Computer Systems （TOCS） 26， 2 （2008）.[8] Vijay Chidambaram、Thanumalayan Sankaranarayana Pillai、An-e drea C. Arpaci-Duseau 和 Remzi H. Arpaci-Dusseau。2013. 乐观崩溃一致性。在 Tuenty-Fourth ACM 操作系统原理研讨会 （SOSP <em>13） 的会议记录中。228243 [9 Vilay ChidambaramDshar Sharma Andrea C Arpaci-Duseon dering.第 1O 届 USENIX 文件和存储技术研讨会 （FAST '12） 的会议记录。加利福尼亚州圣何塞 10 e Wang.2015. Purity：从商用组件构建快速、高度可用的企业级闪存存储。在 2015 年 ACM SIGMOD 数据管理国际会议 （SIGMOD '15） 的会议记录中。ACM，16831694。[11] 布莱恩 F. Cooper、Raghu Ramakrishnan、Utkarsh Srivastava、Adam Silberstein、Philip Bohannon、Hans-Arno Jacobsen、Nick Puz、Daniel Weaver 和 Ramana Yerneni。2008. PNUTS：Yahoo！ 的托管数据服务平台。VLDB Endow 论文集。1、2（2008 年 8 月），12771288 [12] DataStax。2017. DSE 5.1 管理员指南：更改日志记录位置.https：//docs.datastax.com/en/dse/5.1/dse-admin/ tem.在第 4 届 USENIX 存储和文件系统热门话题研讨会 （HotStorage </em>12） 中。[ a （hbsom [18] Grery er d Y Pat 199tttx 操作系统设计和实现会议 USENIX 协会 [19] Sanjay Ghemawat 和 Jef Dean。2011. 水平IDB。http：/leveldb [20 d RDNS compaction-nuance.html.（2017） [21] 罗宾·德·菲尔系统 [22]胡小宇、伊万杰洛斯·埃莱夫塞里乌、罗伯特·哈斯、伊利亚斯·伊利亚迪斯和罗曼·普莱特卡。2009. 基于闪存的固态驱动器中的写入放大分析。在 SYSTOR 2009 会议记录中：以色列实验系统会议。ACM 的。[23]InfuxData， Inc. 2017 年。指标和事件的现代引擎 https： / /<a data-tooltip-position="top" aria-label="http://www.influxdata.com/" rel="noopener" class="external-link" href="http://www.influxdata.com/" target="_blank">www.influxdata.com/</a> - （2017）。</p></div><div><p dir="auto">[24] HV Jagadish Ps Narayan Seidhar Seshadri S Sudashan 以及记录和仓储。在 VLDB，第 97 卷。1625 [25Wiaa J Ah Ah a ohn Reddy， Leif Walsh， et al. 2015.BetrFS：第 13 届 USENIX 文件与存储技术会议 （FAST '15） System.In 右优化写入优化文件。301315. [26 Some e gj ng Son 和 2013 年 USENIX 年度技术会议 （USENIX ATC 13.309320 [27] Jeong-Uk Kang、Jeeseok Hyun、Hyunjoo Maeng 和 Sangyeun Cho。2014. 多流固态驱动器。第 6 届 USENIX 存储和文件系统热门话题研讨会 （HotStor αge *14）。[28 Hyojun Kin， Nifin Stawaland rain Unguran.2012erRge （TOS）8,4 （2012） [ 29 Wwok-Hle Kimaiovong KwmA.Wgogki Baek Beomscok Naene Ahead Logging.在 Tuenty-First 国际会议论文集 30]AH Kihara 和 Satoshi Moriai 的建筑支持会议上。2006. 日志结构文件系统的 Linux 实现。ACM SIGOPS 操作系统评论 40， 3 （2006）， 102107 [31 TJ Kowalki.1990 FreckaThe Unix Fle System Check Program [32] Avinash Lakshman 和 Prashant Malik。2010. Cassandra：一种去中心化的结构化存储系统。ACM SIGOPS 操作系统评论 44， 2 （2010）， 3540。[33]Changman Lee、Dongho Sim、Jooyoung Hwang 和 Sangyeur Cho。2015.F2FS：用于闪存的新文件系统 Storage.In 13ti USENIX 文件和存储技术会议 （FAST '15）。[34] Sungjin Lee， Ming Liu， Sang Woo Jun， Shuotao Xu， Jihong Kim.和 Arvind Arvind。2016. 应用程序管理的 Flash。在第 14 届 USENIX 文件和存储技术会议 （FAST 16339353 [35] 中，Cheng Li、Philip Shilane、Fred Douglis、Darren Sawyer 和 Hyong Shim。2014. 断言 （！已定义（顺序 I/O））。在第 6 届 USENIX 存储和文件系统热门话题研讨会 （HotStorage 14） 中。 36 1 （1977）， 91104 [37] 兰月卢、Thanumalayan Sankaranarayana Pillai、Andrea C Arpaci-Duseau 和 Remzi H Arpaci-Dusseau。2016. WiscKey：在 SSD 敏感型存储中将键与值分开。在第 14 届 USENIX 文件和存储技术会议 （FAST '16） 中。133148 [38] Ao 马、Chris Dragga、Andrea C Arpaci-Dusseau、Remzi H ArpaciDusseau 和 Marshall Kirk Mckusick。2014. ffsck：快速文件系统检查器。ACM 存储汇刊 （TOS） 101 （2014） [39] Leonardo Marmol、Swaminathan Sundararaman、Nisha Talagala 和 Raju Rangaswami。2015. NVMKV：可扩展的轻量级。FTL 感知键值存储。在 2015 年 USEN1X 年度技术会议 （USENIX ATC 15）.207-219 [40] Avantika Mathur、Mingming Cao、Suparna Bhattacharya、Andreas Dilger、Alex Tomas 和 Laurent Vivier。2007. 新的 Ext4 文件系统：现状和未来计划。在 Linuz 研讨会论文集，第 2 卷中。Citeseer，2133 年。[41] 马歇尔·麦库西克、威廉·乔伊、塞缪尔·莱夫勒和罗伯特·法布里。1984. 用于 UNIX 的快速文件系统。ACM 计算机系统汇刊 （TOCS） 2， 3 （1984）， 181- 197 [2] MichalK0r [43] Changwoo Min， Kangnyeon Kim， Hyunjin Cho， Sang-Won Lee 和 Young Ik Eom.2012. SFS：在固态驱动器中随机写入被认为是有害的。在第 10 届 USENIX 文件和存储技术会议 （FAST 12） 中。[44] J. Mohan、R. Kadekodi 和 V Chidambaram。2017. 分析 Linux 文件系统中的 IO 放大。ArXiv 电子打印（2017 年 7 月）。arXiv：cs 的OS/1707 的。08514</p></div><div><hr></div><div><p dir="auto">[45] Rajesh Nishtala、Hans Fugal、Steven Grimm、Marc Kwiatkowski、Herman Lee、Harry C Li、Ryan McElroy、Mike Paleczny、Daniel Peek、Paul Saab 等人，2013 年。在 Facebook 扩展 Memcache。在第 10 届 USENIX 网络系统设计和实现研讨会 （NSDI 13） 中385398 [46] Patrick Oneil、Edward Cheng、Dieter Gawlick 和 Elizabeth Oneil。1996. 日志结构合并树 （LSM 树）。Act Informatica （1996 年）。[4] MikOwens 和 Grant Allen201 SQLite Srionale-Fere 和 Angelos Bilas。2016. Tucana：快速高效的纵向扩展键值存储的设计和实现。2016 年 USENIX 年度技术会议 （USENIX ATC '16）。[49An siars latency Storage Devices.ACM SIGOPS Operating Systems Review 50， 1 （2017）， 311. 50cth Jh P2002mon 加利福尼亚大学圣克鲁斯分校。[51] Tanalakya Vi Chmaram Duseau 和 Remzi H Arpaci-Dusseau。2014. 并非所有文件系统都是平等的：关于制作崩溃一致性应用程序的复杂性。在第 1I 届 USENIX 操作系统设计和实施会议论文集 OSDI14 中）。BroomfieldCO，433-448 [52 Pandian Rant R ohan Kadekod.Vjlay Chidymbaerems 和 wtae 吞吐量和 wtae 吞吐量和减少写入放大在第 26 届 ACM 操作系统原理会议记录 （SOSP 17） 的键值 Stores.In 论文集中。中国上海 [53]Remzi H. Arpaci-Dusseau 和 Andrea C. Arpaci-Dusseau.2015 年崩溃一致性：FSCK 和日志。<a data-tooltip-position="top" aria-label="http://pages.cs.wisc%E3%80%82edu/remzi/OSTEP/file-journaling.pdf%E3%80%82%EF%BC%882015%EF%BC%89" rel="noopener" class="external-link" href="http://pages.cs.wisc%E3%80%82edu/remzi/OSTEP/file-journaling.pdf%E3%80%82%EF%BC%882015%EF%BC%89" target="_blank">http://pages.cs.wisc。edu/remzi/OSTEP/file-journaling.pdf。（2015）</a>. [54] 凯任和加思吉布森。2013. TABLEFS：提高本地文件系统中的元数据效率。2013 年 USENIX 年度技术会议 （USENIX ATC '13）。145156 [55 Ohad oacik dris M0139,3 （2013） [56] Mendel Rosenblum 和 John K Ousterhout。1992. 日志结构文件系统的设计和实现。ACM 计算机系统汇刊 （TOCS） 10， 1 （1992）。2652. [57] 罗素·西尔斯和拉古·拉马克里希南。2012. bLSM：通用日志结构化合并树。在 2012 年 ACM SIGMOD 数据管理国际会议 （SIGMOD *12） 的论文集中。ACM， 217228 [58 DJ Shebhy 和 DS/mith 2010 Bfeask A Log-Sru（tured Hash [59 Prg Workload-Independent Storage with VT-Trees.在 I1th USENIX 文件和存储技术会议 （FAST '13） 中。17- 30.[60] Kent Smith.2011 垃圾收集 SandFore， Flash Memory [61] SQ 2016it vhttps//ww [62Micrk Data [63 Pure torage.2017.Prom BieDatatoBis Itell ence htps： [64] ZaoHui Tangand Jamine Maclen.205. 数据挖掘 unth [65] okutek.伊恩。2013TokuDB： MySQL Performance， MariaDB [66] Mehul Nalin Vora.2011. 用于大规模数据的 Hadoop-HBase。2011 年计算机科学与 nefuwork 技术国际会议 （ICCSNT），第 1 卷。IEEE， 601605 [67] 王鹏、孙光宇、宋江、欧阳健、林世丁、张晨和丛健。2014 年。在 OpenChannel SSD 上基于 LSM 树的键值存储的高效设计和实现。在第九届欧洲计算机系统会议 （EuroSys '14） 的论文集中。ACM [68]Ext4 Wiki.2011. Ext4 写入请求的生命周期.https：/ext4.wiki kernel.org/index.php/Life_of_an_ext4_write_request。(2011).</p></div><div><p dir="auto">[69] 吴兴波， 徐月海， 邵子力， 江松.2015. LSM-trie：基于 LSM 树的小数据超大型键值存储 2015 年 USENIX 年度技术会议 （USENIX ATC 15）。[70] Jingpei Yang， Ned Plasson， Greg GDis， Nisha Talagala，g 和 My Log.第 2 届 NVM/闪存与操作系统和工作负载交互研讨会 （INFLOW *14）</p></div><div class="mod-footer"></div></div></div></div></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="graph-view-wrapper"><div class="sidebar-section-header">Interactive Graph</div><div class="graph-view-placeholder">
		<div class="graph-view-container">
			<div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div>
			<canvas id="graph-canvas" class="hide" width="512px" height="512px"></canvas>
		</div>
		</div></div><div class="tree-container mod-root nav-folder tree-item outline-tree" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">Table Of Contents</span><button class="clickable-icon collapse-tree-button" aria-label="Collapse All"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area tree-item-children nav-folder-children"><div class="tree-item mod-tree-folder nav-folder mod-collapsible is-collapsed" style="display: none;"></div><div class="tree-item" data-depth="1"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#用于大规模键值存储的 LSM 树托管存储"><div class="tree-item-contents heading-link" heading-name="用于大规模键值存储的 LSM 树托管存储"><span class="tree-item-title">用于大规模键值存储的 LSM 树托管存储</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#KEYWORDS"><div class="tree-item-contents heading-link" heading-name="KEYWORDS"><span class="tree-item-title">KEYWORDS</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#抽象"><div class="tree-item-contents heading-link" heading-name="抽象"><span class="tree-item-title">抽象</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#ACM_Reference_Format:_Fei_Mei,_Qiang_Cao,_Hong_Jiang,_and_Lei_Tian._2017._LSM-tree"><div class="tree-item-contents heading-link" heading-name="ACM Reference Format: Fei Mei, Qiang Cao, Hong Jiang, and Lei Tian. 2017. LSM-tree"><span class="tree-item-title">ACM Reference Format: Fei Mei, Qiang Cao, Hong Jiang, and Lei Tian. 2017. LSM-tree</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#1引言"><div class="tree-item-contents heading-link" heading-name="1引言"><span class="tree-item-title">1引言</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#CCS_CONCEPTS"><div class="tree-item-contents heading-link" heading-name="CCS CONCEPTS"><span class="tree-item-title">CCS CONCEPTS</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#2背景和动机"><div class="tree-item-contents heading-link" heading-name="2背景和动机"><span class="tree-item-title">2背景和动机</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#2.1_LSM-Tree_and_LevelDB"><div class="tree-item-contents heading-link" heading-name="2.1 LSM-Tree and LevelDB"><span class="tree-item-title">2.1 LSM-Tree and LevelDB</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#2.2文件系统上的_LSM-Tree"><div class="tree-item-contents heading-link" heading-name="2.2文件系统上的 LSM-Tree"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">2.2文件系统上的 LSM-Tree</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#3设计与实施"><div class="tree-item-contents heading-link" heading-name="3设计与实施"><span class="tree-item-title">3设计与实施</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#2.3Motivation"><div class="tree-item-contents heading-link" heading-name="2.3Motivation"><span class="tree-item-title">2.3Motivation</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#3.1磁盘布局"><div class="tree-item-contents heading-link" heading-name="3.1磁盘布局"><span class="tree-item-title">3.1磁盘布局</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#3.2LSM-tree_Managed_Storage"><div class="tree-item-contents heading-link" heading-name="3.2LSM-tree Managed Storage"><span class="tree-item-title">3.2LSM-tree Managed Storage</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#3.3Log_Write"><div class="tree-item-contents heading-link" heading-name="3.3Log Write"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">3.3Log Write</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#图_9：左侧的包装对象_（WoL）_和右侧的包装对象_（WOR）。"><div class="tree-item-contents heading-link" heading-name="图 9：左侧的包装对象 （WoL） 和右侧的包装对象 （WOR）。"><span class="tree-item-title">图 9：左侧的包装对象 （WoL） 和右侧的包装对象 （WOR）。</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#3.4Internal_Fragmentation"><div class="tree-item-contents heading-link" heading-name="3.4Internal Fragmentation"><span class="tree-item-title">3.4Internal Fragmentation</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#3.5实现"><div class="tree-item-contents heading-link" heading-name="3.5实现"><span class="tree-item-title">3.5实现</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#3.6_Scalability"><div class="tree-item-contents heading-link" heading-name="3.6 Scalability"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">3.6 Scalability</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#4_EVALUATION"><div class="tree-item-contents heading-link" heading-name="4 EVALUATION"><span class="tree-item-title">4 EVALUATION</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#4.1Environment_Setup"><div class="tree-item-contents heading-link" heading-name="4.1Environment Setup"><span class="tree-item-title">4.1Environment Setup</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#4.2Write_Performance"><div class="tree-item-contents heading-link" heading-name="4.2Write Performance"><span class="tree-item-title">4.2Write Performance</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#4.3Read_Performance"><div class="tree-item-contents heading-link" heading-name="4.3Read Performance"><span class="tree-item-title">4.3Read Performance</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="1"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#4.4恢复"><div class="tree-item-contents heading-link" heading-name="4.4恢复"><span class="tree-item-title">4.4恢复</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#4.5Space_Utilization"><div class="tree-item-contents heading-link" heading-name="4.5Space Utilization"><span class="tree-item-title">4.5Space Utilization</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#4.6预先分配的文件空间开销"><div class="tree-item-contents heading-link" heading-name="4.6预先分配的文件空间开销"><span class="tree-item-title">4.6预先分配的文件空间开销</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#5相关工作"><div class="tree-item-contents heading-link" heading-name="5相关工作"><span class="tree-item-title">5相关工作</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#5.1写入优化的数据结构"><div class="tree-item-contents heading-link" heading-name="5.1写入优化的数据结构"><span class="tree-item-title">5.1写入优化的数据结构</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#5.2Optimizations_on_LSM-trees"><div class="tree-item-contents heading-link" heading-name="5.2Optimizations on LSM-trees"><span class="tree-item-title">5.2Optimizations on LSM-trees</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#5.3绕过存储堆栈层"><div class="tree-item-contents heading-link" heading-name="5.3绕过存储堆栈层"><span class="tree-item-title">5.3绕过存储堆栈层</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#5.4New_Storage_Technology"><div class="tree-item-contents heading-link" heading-name="5.4New Storage Technology"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">5.4New Storage Technology</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#6CONCLUSION"><div class="tree-item-contents heading-link" heading-name="6CONCLUSION"><span class="tree-item-title">6CONCLUSION</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#ACKNOWLEDGMENTS"><div class="tree-item-contents heading-link" heading-name="ACKNOWLEDGMENTS"><span class="tree-item-title">ACKNOWLEDGMENTS</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="读书笔记\曹老师2019年论文\2024年9月12日_正文.html#引用"><div class="tree-item-contents heading-link" heading-name="引用"><span class="tree-item-title">引用</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div></div></div></div></div></div><script defer="">let rs = document.querySelector(".sidebar-right"); rs.classList.add("is-collapsed"); if (window.innerWidth > 768) rs.classList.remove("is-collapsed"); rs.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-right-width"));</script></div></div></body></html>